# AI_theory

![Python](https://img.shields.io/badge/python-v3.9.4+-blue.svg)
![Numpy](https://img.shields.io/badge/numpy-1.8.0rc1+-yellow)
![Matplotlib](https://img.shields.io/badge/matplotlib-1.3.1+-red)

## Basic Overview

This repository is about the use of artificial intelligence theory and related libraries.

---

# GDA

## gradient descent ì˜ ëª©ì 

- í•¨ìˆ˜ì˜ ìµœì†Œê°’ì„ ì°¾ëŠ” ë¬¸ì œì—ì„œ ì£¼ë¡œ í™œìš©
- ë¯¸ë¶„ê³„ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ì»´í“¨í„°ë¡œ êµ¬í˜„í•˜ëŠ”ê²ƒë³´ë‹¤ gradient descentë¡œ ë” ì‰½ê²Œ êµ¬í˜„ ê°€ëŠ¥

## gradient descent ë€?

- í•™ìŠµë¥ ê³¼ ì†ì‹¤í•¨ìˆ˜ì˜ ìˆœê°„ê¸°ìš¸ê¸°(gradient)ë¥¼ ì´ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜(weight)ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ë²•
- ë¯¸ë¶„ì˜ ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•˜ì—¬ ë„í‘œì˜ ì˜¤ì°¨ë“¤ì„ ë¹„êµí•˜ê³  ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™ì‹œí‚¤ëŠ” ë°©ë²•

---

## problem

<img src="./img/Problem.png" >

```python
# cost function & gradient
def f(x): return (x[0] - 3) ** 2 + (x[1]-5) ** 2 + 10
def grad(x): return 2 * (x - np.array([3, 5]))
```

## ì´ˆê¸° ì„¤ì •

```python
# í•™ìŠµìœ¨(lr) ë° ë©ˆì¶¤ ì¡°ê±´(pause)
iter_count = 0
lr = 0.2
pause = 0.01
x = np.array([10, 20])        # x ì´ˆê¸° ì„¤ì •ê°’
x_vals = [x.tolist()]         # array ë¥¼ list ë¡œ ë°˜í™˜
cost_vals = [f(x)]
prev_cost = f(x)

```

## gradient descent ìˆ˜ì‹

<img src="./img/SGDìˆ˜ì‹.png" >

```python
x = x - lr * grad(x)
```

---

## ìµœì¢… ìˆ˜ì‹

```python
while True:
    iter_count += 1
    x = x - lr * grad(x)
    curr_cost = f(x)
    print("%3d-th iteration: x = [%0.4f, %0.4f]\
        cost = %0.4f" % (iter_count, x[0], x[1], f(x)))

    # ë©ˆì¶¤ ì¡°ê±´ ì„¤ì •
    if curr_cost > prev_cost or np.abs(curr_cost - prev_cost) < pause:
        break

    x_vals.append(x.tolist())
    cost_vals.append(curr_cost)
    prev_cost = curr_cost

    print("Final result : x = [%0.4f , %0.4f], cost = %0.4f \
    at iteration = %d\n" % (x[0], x[1], f(x), iter_count))
```

## ê²°ê³¼

```python
[output]
[10 20]
  1-th iteration: x = [7.2000, 14.0000]       cost = 108.6400
  2-th iteration: x = [5.5200, 10.4000]       cost = 45.5104
  3-th iteration: x = [4.5120, 8.2400]        cost = 22.7837
  4-th iteration: x = [3.9072, 6.9440]        cost = 14.6021
  5-th iteration: x = [3.5443, 6.1664]        cost = 11.6568
  6-th iteration: x = [3.3266, 5.6998]        cost = 10.5964
  7-th iteration: x = [3.1960, 5.4199]        cost = 10.2147
  8-th iteration: x = [3.1176, 5.2519]        cost = 10.0773
  9-th iteration: x = [3.0705, 5.1512]        cost = 10.0278
 10-th iteration: x = [3.0423, 5.0907]        cost = 10.0100
 11-th iteration: x = [3.0254, 5.0544]        cost = 10.0036
Final result : x = [3.0254 , 5.0544], cost = 10.0036     at iteration = 11
```

<img src="./img/SGD.png" width="500" height="500">
<img src="./img/SGD2.png" width="500" height="500">

---

# IRIS Classification

## Logistic Regression ë€?

- íšŒê·€ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ê°€ ì–´ë–¤ ë²”ì£¼ì— ì†í•  í™•ë¥ ì„ 0ì—ì„œ 1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì˜ˆì¸¡
- ê·¸ í™•ë¥ ì— ë”°ë¼ ë” ë†’ì€ ë²”ì£¼ì— ì†í•˜ëŠ”ê²ƒì„ ë¶„ë¥˜í•´ì£¼ëŠ” ì§€ë„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜.

---

## êµ¬í˜„ë‚´ìš©

- IRIS Datasetì˜ 3ê°€ì§€ì˜ íŠ¹ì§•ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶“ê½ƒì˜ ì¢…ë¥˜ë¥¼ ì˜ˆì¸¡í•œë‹¤.
- ì‹¤ì œ ë¶“ê½ƒì˜ ì¢…ë¥˜ì™€ ì˜ˆì¸¡ëœ ì¢…ë¥˜ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë¹„êµí•  ìˆ˜ ìˆê²Œ í•œë‹¤.

---

## ì´ˆê¸°ì„¤ì •

```python
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=0, train_size=0.7)

X_train = X_train[:, [0, 2, 3]]
X_test = X_test[:, [0, 2, 3]]
# Sepal length , petal length, petal width ì¶”ì¶œ
```

---

## ë¡œì§€ìŠ¤í‹±íšŒê·€ì— ì˜í•œ í›ˆë ¨

```python
log_reg = LogisticRegression(random_state=0).fit(X_train, y_train)
```

---

## ë°ì´í„° ì‹œê°í™” (Train Data)

```python
fig = plt.figure()

ax = fig.add_subplot(1, 2, 1, projection='3d')
ax.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],
           X_train[y_train == 0, 2], c="r", label='setosa')
ax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],
           X_train[y_train == 1, 2], c="g", label='versicolor')
ax.scatter(X_train[y_train == 2, 0], X_train[y_train == 2, 1],
           X_train[y_train == 2, 2], c="b", label='virginica')

plt.legend(), plt.grid(), plt.title("Iris data training set")
ax.set_xlabel("Sepal length"), ax.set_ylabel(
    "Petla length"), ax.set_zlabel("Petla width")


ax = fig.add_subplot(1, 2, 2, projection='3d')
ax.scatter(X_train[correct_train_index, 0], X_train[correct_train_index, 1],
           X_train[correct_train_index, 2], c="r", marker="o", label="Correct")
ax.scatter(X_train[false_train_index, 0], X_train[false_train_index, 1],
           X_train[false_train_index, 2], c="b", marker="x", label="False")

plt.legend(), plt.grid(), plt.title("Iris data training set")
ax.set_xlabel("Sepal length"), ax.set_ylabel(
    "Petla length"), ax.set_zlabel("Petla width")


plt.show()

```

<img src="./img/IRIS_traindata.png" width="1000" height="500">

---

## ë°ì´í„° ì‹œê°í™” (Test Data)

```python
plt.rcParams["figure.figsize"] = (12, 4)

fig = plt.figure()

ax = fig.add_subplot(1, 2, 1, projection='3d')
ax.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1],
           X_test[y_test == 0, 2], c="r", label='setosa')
ax.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1],
           X_test[y_test == 1, 2], c="g", label='versicolor')
ax.scatter(X_test[y_test == 2, 0], X_test[y_test == 2, 1],
           X_test[y_test == 2, 2], c="b", label='virginica')

plt.legend(), plt.grid(), plt.title("Iris data testing set")
ax.set_xlabel("Sepal length"), ax.set_ylabel(
    "Petla length"), ax.set_zlabel("Petla width")


ax = fig.add_subplot(1, 2, 2, projection='3d')
ax.scatter(X_test[correct_test_index, 0], X_test[correct_test_index, 1],
           X_test[correct_test_index, 2], c="r", marker="o", label="Correct")
ax.scatter(X_test[false_test_index, 0], X_test[false_test_index, 1],
           X_test[false_test_index, 2], c="b", marker="x", label="False")

plt.legend(), plt.grid(), plt.title("Iris data testing set")
ax.set_xlabel("Sepal length"), ax.set_ylabel(
    "Petla length"), ax.set_zlabel("Petla width")
```

<img src="./img/IRIS_testdata.png" width="1000" height="500">

---

## ì„±ëŠ¥ í‰ê°€

```python
print("Testing set performance:", log_reg.score(X_train, y_train))
print("Test set performance:", log_reg.score(X_test, y_test))

[output]
Testing set performance: 0.9809523809523809
Test set performance: 0.9777777777777777

```

---

# MLP(Multilayer Perceptron)

## MLP(Multilayer Perceptron) ë€?

- MLPëŠ” ì¼ë ¨ì˜ ì…ë ¥ì—ì„œ ì¼ë ¨ì˜ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” í”¼ë“œ í¬ì›Œë“œ ì¸ê³µ ì‹ ê²½ë§ì´ë‹¤.
- MLPëŠ” ì…ë ¥ ë ˆì´ì–´ì™€ ì¶œë ¥ ë ˆì´ì–´ ì‚¬ì´ì— ë°©í–¥ ê·¸ë˜ì¸ ë¡œ ì—°ê²°ëœ ì—¬ëŸ¬ ì…ë ¥ ë…¸ë“œ ë ˆì´ì–´ë¥¼ íŠ¹ì§•ìœ¼ë¡œí•œë‹¤. MLPëŠ” ë„¤íŠ¸ì›Œí¬ êµìœ¡ì„ ìœ„í•´ ì—­ì „íŒŒë¥¼ ì‚¬ìš©í•œë‹¤. MLPëŠ” ë”¥ëŸ¬ë‹ ë°©ë²•ì´ë‹¤.

---

## êµ¬í˜„ë‚´ìš©

- [1] Flatten-Dense(300)-Dense(50)-Softmax(10)
- [2]Flatten-Dense(300)-Dense(100)-Softmax(10)
- [1],[2] ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•˜ê¸°

## ì´ˆê¸° ì„¤ì •

```python
# ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

#ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
fashion_mnist = tf.keras.datasets.fashion_mnist
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()

#ë°ì´í„° ì´ë¯¸ì§€ í™•ì¸
class_names = ["T-shirt/top" , "Trouser", "Pullover", "Dress", "Coat",
               "Sandal" , "Shirt", "Sneaker", "Bag", "Ankle boot"]

plt.figure(figsize=(10,5))

for c in range(5):
    plt.subplot(1,5,c+1)
    plt.imshow(X_train_full[c], cmap ="gray")
    plt.title(class_names[y_train_full[c]])
    plt.axis("off")
plt.show()

#í•™ìŠµ:ê²€ì¦:í…ŒìŠ¤íŠ¸ ë¶„ë¥˜
X_valid, X_train = X_train_full[:5000]/ 255.,  X_train_full[5000:] /255.
y_valid, y_train = y_train_full[:5000] , y_train_full[5000:]
X_test = X_test /255.

print(" í•™ìŠµ:ê²€ì¦:í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê°œìˆ˜  = {}:{}:{}".format(len(X_train)
                                              ,len(X_valid),len(X_test)))

```

---

## ëª¨ë¸ êµ¬í˜„

```python
#Build a model

# (2) ë¶„ë¥˜ê¸°
np.random.seed(42)
tf.random.set_seed(42)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))
model.add(tf.keras.layers.Dense(100, activation = "relu"))
model.add(tf.keras.layers.Dense(50, activation = "relu"))
model.add(tf.keras.layers.Dense(10, activation = "softmax"))

model.summary()
"""
# (1) ë¶„ë¥˜ê¸°
np.random.seed(42)
tf.random.set_seed(42)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))
model.add(tf.keras.layers.Dense(300, activation = "relu"))
model.add(tf.keras.layers.Dense(100, activation = "relu"))
model.add(tf.keras.layers.Dense(10, activation = "softmax"))

model.summary()
"""
```

---

## ëª¨ë¸ í•™ìŠµ

```python
#Training model, fit()
#ëª¨ë¸ì˜ weightsì™€ biasê°’ì„ ê²½ì‚¬í•˜ê°•ë²•(GDA)ë¥¼ ì´ìš©í•´ì„œ í•™ìŠµì„ í†µí•´ ê²°ì •
# verbose -> (1 =ìì„¸í•˜ê²Œ), (2 = ê°„ëµí•˜ê²Œ)
history = model.fit(X_train, y_train, epochs=30,batch_size = 32, verbose =2, validation_data = (X_valid, y_valid))
```

---

## ì‹œê°í™”

```python
print("history params = ", history.params)
print("history epoch = ", history.epoch)
print("history keys = ", history.history.keys())

import pandas as pd

pd.DataFrame(history.history).plot(figsize = (8,5))
plt.grid()
plt.gca().set_ylim(0,1)
plt.show()
```

<img src="./img/MLP.png" width="1000" height="500">

---

## ì„±ëŠ¥ í‰ê°€

```python
#Model Evaluation

model.evaluate(X_test,y_test)

[output]
313/313 [==============================] - 1s 2ms/step - loss: 0.3363 - accuracy: 0.8826
[0.3363315761089325, 0.8826000094413757]

```

---

## Contributing

Let's connect ğŸ‘¨â€ğŸ’» and forge the future together.ğŸ˜âœŒ

**Check the Repositories and don't forget to give a star.** ğŸ‘‡

:star: From [S-jooyoung](https://github.com/S-jooyoung)
